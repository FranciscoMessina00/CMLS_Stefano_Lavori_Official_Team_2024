The proposed application, provides a comprehensive and flexible environment for real-time sound creation and manipulation, offering users a wide range of tools and techniques to explore the possibilities of digital sound synthesis. Based on a hybrid approach that integrates concepts of additive synthesis, digital signal manipulation, and pattern generation, the application allows for a wide range of sonic and musical experiments.
In terms of sound synthesis, the application predominantly employs additive synthesis, specifically designed to simulate the vibration of a string. This method approach is a simplification of modal synthesis, where complex sound waves are approximated by summing multiple sine waves and involves generating complex sounds by summing multiple sinusoidal signals, each representing a harmonic component of the sound. The SynthDef defines the frequencies and amplitudes of these sinusoids based on a fundamental frequency and user-defined parameters such as stiffness and pose. This allows users to create rich and detailed harmonic spectra, enabling the creation of nuanced and dynamic sounds.
Once the sound is generated through additive synthesis, the application includes a granulator for further sound manipulation, and it allows for the creation of complex sounds by combining sinusoidal signals through audio grains extracted from a buffer. The flexibility of the SynthDef allows users to creatively modulate and manipulate sounds through the application of filters, envelopes, and other effects.
Additionally, the application offers a wide range of tools for generating and manipulating musical patterns. Using a combination of timed sequences and generative patterns, users can intuitively and dynamically control the trigger, duration, frequency, and other parameters of sounds. This approach enables the creation of complex and varied musical structures, suitable for both live performance and studio music composition.
It is important to understand the philosophy of our instrument: the process of creating rhythms and melodies is purely generative, giving the user the freedom to explore uncharted territories by interacting with a system designed to suggest new directions for the musician.
The extensibility of the application make it suitable for a wide range of musical contexts, allowing users to explore new frontiers in interactive music composition. By giving the user control over randomness, we have created a system that excels in experimenting with polyrhythms and complex melodies, while also providing the ability to improvise solo or with other musicians

Additive Synthesis: Simulating String Vibration
In this application, the string's vibration is simulated by generating an array of frequencies and amplitudes corresponding to the harmonic series of a vibrating string. We begin with the ideal implementation of a string, where each frequency is a multiple of the fundamental frequency of the string. The amplitudes of each frequency are influenced by factors such as the plucking position on the string (p_pose). These amplitudes are calculated using a formula based on the d’alambert equation that simulates the energy distribution of a real vibrating string, resulting in a more realistic sound:
(Formula ampiezze)
We also considered the condition where the stiffness of the string is relevant. As an effect, each frequency is slightly detuned to simulate the natural imperfections in a real string. The detuning is controlled by a stiffness parameter, which adjusts both a constant detune applied to all harmonic frequencies and a variable part that depends on the harmonic number. This also considers the real theory based on the d’Alambert equation with a fourth derivative component that takes the stiffness into consideration:
(Formula equazione differenziale con quarto grado)
As a solution we come up with a new formula for calculating each frequency harmonic:
(Formula frequenze deviate)
The value of beta that gave us a more realistic sound was 0,011.
This additive synthesis approach was used because much simpler to implement compared to real implementation of modal synthesis, even though it may increase CPU usage. To reduce the stress of the CPU we can reduce the number of harmonics that our Synth calculates.


Pattern Generation
The pattern generation process in the application is a crucial component that shapes the rhythmic and melodic structure of the sound. This process involves creating and manipulating rhythmic patterns, note sequences, and modulation values.
•	Euclidean Rhythm Generation
At the core of the rhythmic pattern generation is the Euclidean rhythm algorithm. This algorithm distributes a specified number of beats as evenly as possible across a given number of steps. For example, if you want to distribute 5 beats over 16 steps, the Euclidean algorithm ensures that these beats are spaced as evenly as possible, creating interesting and often complex rhythmic patterns.
The steps parameter defines the total length of the rhythm cycle, while the pulses parameter determines how many of these steps will contain beats. The rotation parameter then shifts the pattern, effectively changing the starting point of the cycle, which adds further variation to the rhythm. This rotation can be controlled and modified in real-time, allowing dynamic changes to the rhythm during performance.

•	Combination with Random Sequences
In addition to the Euclidean rhythm, the application integrates randomness to further enhance the variability and unpredictability of the patterns. A random sequence is generated and updated continuously, with each element in the sequence being a binary value (0 or 1), determined by a coin flip biased by the probability parameter. This sequence is then combined with the Euclidean rhythm using logical operations such as OR, AND, XOR, and NAND.
These operations determine how the two patterns interact. For instance, using an OR operation means that a step will be active (a beat will occur) if either the Euclidean rhythm or the random sequence has a beat at that step. This combination can create highly intricate and evolving rhythmic patterns that blend deterministic structure with stochastic elements.

•	Looping and Modulation
The application also supports looping, allowing sections of the generated patterns to be repeated. Users can enable or disable looping for different patterns, and they can specify the length of these loops. This feature is especially useful for creating cyclical rhythmic motifs or repetitive melodic phrases.
Modulation is another key aspect of the pattern generation process. Parameters such as note velocities and modulation values (which affect timbral changes) are also generated dynamically. These values can be looped and modulated over time, adding expressive dynamics and evolving textures to the sound.
The modulation loop operates similarly to the rhythmic loop, where a sequence of modulation values is continuously rotated and updated. This sequence can be influenced by Gaussian distributions, allowing for controlled randomness in the modulation values. By adjusting parameters like standard deviation and expected value, users can fine-tune the range and variability of the modulation. In summary, the pattern generation in this application is a sophisticated process that combines deterministic algorithms, such as Euclidean rhythms, with stochastic elements, like random sequences and Gaussian-distributed modulation values. These patterns are dynamically manipulated and can be looped, rotated, and modulated, providing a rich and versatile framework for generative music.

Graphical User Interface (GUI) Implementation:
The graphical user interface (GUI) in the provided code is crucial for facilitating user interaction and control over sound synthesis and processing operations. Implemented using the ControlP5 library in Processing, this GUI comprises various interactive elements such as toggles, knobs, and sliders. The left side and central part of the view contains all the parameters that control the generative synth in SuperCollider, while the right side contains all the controls of the Folder&Distortion and Flanger VST3 plugins.
Toggle Buttons for Mode Selection:
At the top left of the GUI window, horizontal toggle buttons are prominently displayed, each representing a distinct operational mode. These buttons allow users to seamlessly toggle the loop modes, enhancing flexibility and ease of use.

Knobs for Parameter Adjustment:
The GUI features several blocks containing knobs, each dedicated to controlling specific parameters related to sound synthesis and processing. These knobs, created from the ControlP5 library, enable users to finely adjust the parameters of the application.

Slider Controls for Two-Dimensional Parameters:
In addition to knobs, the GUI incorporates slider controls for manipulating two-dimensional parameters. These sliders, provide users with an intuitive way to adjust parameters within a defined range by visually positioning a point on the interface. They control the expected value (x direction) and standard deviation (y direction) of the distribution of notes, velocity, and modulation (plucking position of the string) respectively.

Control section
In the middle part of the GUI we can see three different controls which allows the user to stop or resume the generative synthesiser. In addition the user can control the BPM of the sequence, while in the bottom of the GUI the user can observe the shape of the Euclidean rhythm, which allows for a visual representation of the sequence.
Overall, the GUI enhances user interaction by offering tactile controls for adjusting parameters and exploring various sonic possibilities within the Processing environment. It provides a visually appealing and user-friendly interface for controlling sound synthesis and processing operations.

Communication between Processing and SuperCollider via OSC

The application uses the OscP5 library in Processing to manage communication via the OSC (Open Sound Control) protocol. This allows for sending messages between Processing and SuperCollider for real-time control of sound parameters.

In the setup() method, an OSC connection is established between Processing and SuperCollider. Processing listens on port 24 for incoming OSC messages and sends messages to SuperCollider at the localhost address (127.0.0.1) on port 57120.
The Processing GUI allows users to interact with various controls, such as knobs and sliders. When the user interacts with one of these controls, a callback event is generated. This event creates an OSC message containing the current values of all the controls and sends it to SuperCollider. SuperCollider receives these messages and updates the synthesizer parameters in real time based on the received values.
Additionally, Processing can receive OSC messages from SuperCollider. When Processing receives an OSC message, it extracts the values contained in the message and uses them to update the GUI or for other necessary processing.
This bidirectional communication between Processing and SuperCollider via OSC allows for smooth and dynamic control of sound operations, providing users with a powerful platform for real-time sound creation and manipulation.

Sensor-Controlled Slider2D Integration with Bela

The integration of external sensors with the Slider2D elements in the GUI is facilitated by Bela, serving as a crucial intermediary platform. When OSC messages with the address pattern "/sensor" are received in the oscEvent(OscMessage theOscMessage) function, Bela interprets and processes these messages. It acts as the bridge between the external sensors and the Processing environment, translating the sensor data into OSC messages that the GUI can understand.
Specifically, Bela communicates with the external sensors, collecting data such as position, orientation, or any other relevant parameters. It then packages this data into OSC messages and sends them to the Processing environment. Within Processing, the oscEvent function receives these OSC messages, extracts the necessary data, and applies it to dynamically adjust the positions and configurations of the Slider2D controls.
By leveraging Bela's capabilities, the GUI achieves real-time responsiveness to sensor input. This integration enables users to interact intuitively with the Slider2D controls, manipulating two-dimensional parameters within the Processing environment based on the data received from external sensors. Thus, Bela plays a pivotal role in enhancing the user experience by providing seamless and precise control over specific parameters in the GUI through sensor input.

JUCE-Based VST3 Plugin

In the first VST3 plugin developed using the JUCE framework, we've implemented two essential audio effects: distortion and folding. These effects are fundamental in shaping and enhancing audio signals, providing users with creative tools for sound manipulation.

The distortion effect implemented in our plugin introduces harmonic richness and saturation to audio signals. This effect is achieved by non-linearly amplifying the input signal, causing signal clipping and introducing harmonic overtones. By adjusting parameters such as the distortion amount, users can control the intensity of the effect, ranging from subtle warmth to aggressive saturation.

The folding effect, also known as wavefolding, is a unique form of distortion that creates complex waveform shapes by folding parts of the signal back onto itself. This effect adds harmonically rich overtones and creates unique timbral characteristics in audio signals. In our plugin, the folding effect is achieved by dynamically manipulating the amplitude of the audio signal, folding it back upon exceeding predefined thresholds. Users can adjust parameters such as the folding amount to control the intensity of folding, allowing for versatile sound sculpting possibilities.

In the second plugin, we've implemented the flanger effect to offer users a versatile audio modulation tool for creating dynamic and immersive soundscapes.

The flanger effect is a classic modulation effect characterized by its sweeping, comb-filter-like sound. It works by modulating a delayed version of the input signal and mixing it with the original signal, resulting in a distinct sweeping effect with notches in the frequency spectrum. 

Key Parameters:

Wave Type: Allows users to select the waveform shape used for modulation, offering options such as sine, square, triangle, or custom waveforms.
Rate: Controls the speed of modulation, determining how quickly the flanger sweeps through the audio spectrum.
Depth: Adjusts the intensity of modulation, controlling the amplitude of the modulated signal.
Feedback: Sets the feedback level of the effect, influencing the amount of signal fed back into the modulation loop.
Width: Determines the width of the modulation, controlling the range of frequencies affected by the flanger effect.
Dry/Wet Mix: Balances the ratio between the dry (original) and wet (processed) signals, allowing users to blend the effect with the dry signal seamlessly.
Color: Adjusts the tonal character of the effect, offering tonal shaping options to match the desired sonic aesthetic.
Stereo Spread: Controls the stereo width of the effect, allowing users to adjust the spatial distribution of the modulated signal.
Modulation Process:
The flanger effect is achieved by modulating a delayed version of the input signal using an LFO (Low-Frequency Oscillator). The LFO generates a periodic waveform, such as a sine, square, or triangle wave, which is then used to modulate the delay time of the delayed signal. The modulated delayed signal is mixed with the original signal, resulting in the characteristic sweeping effect of the flanger.

By integrating these effects into our plugin, we provide users with powerful tools for shaping and enhancing their audio productions.
